---
title: "La necesidad de comprimir datos en control estadístico multivariable"
---

###### **NOTA:** La siguiente disertación no pretende, de ninguna manera, sustituir el cuerpo formal de conocimientos existente en este campo. Su propósito es introducir al lector no experimentado en la terminología, la simbología y la lógica que explican y sustentan estos procedimientos.

###### Aunque el texto es riguroso desde el punto de vista matemático y metodológico, no es exhaustivo por razones de espacio y de propósito. Los libros de texto existen precisamente para cubrir ese nivel de detalle.

###### Se exhorta al lector interesado a consultar y estudiar con detenimiento las fuentes y referencias especializadas si desea profundizar en los fundamentos teóricos que respaldan estos métodos.

## ¿Comprimir Datos?

En una sección anterior se mencionó que para ser construidos y aplicados de manera confiable, los **modelos empíricos multivariables** requieren **grandes cantidades de datos comprimibles**.

Antes de iniciar, conviene preguntarnos: ¿qué sucede al trabajar formalmente con datos, particularmente cuando son multivariables?

El propósito es tomar decisiones acertadas. Para ello, existen dos submetodologías:

1.  El análisis estadístico (o algorítmico) de los datos.
2.  El análisis gráfico de los datos.

Ambas trabajan juntas y para que cumplamos el sentido formal, deben aplicarse en ese orden.

Sin embargo, incluso antes de analizar cualquier conjunto de números y variables, existe un requisito previo que debemos satisfacer y podemos formularlo con una pregunta: ¿cuál es el fundamento metodológico que debemos seguir y respetar para que nuestra labor analítica sea científicamente válida?

O en términos más coloquiales: ¿cómo vamos a proceder y por qué elegimos hacerlo de esa manera?

En el caso concreto de la construcción de procedimientos de control estadístico para procesos fisicoquímicos multivariables, el mecanismo formal ya existe desde hace décadas. Y todo comienza con una tabla de datos que pueden estar conformados por un gran número de **observaciones** ( $N$ ), un gran número de **variables por observación** ($M$ ), o por ambos simultáneamente. En el caso de los procesos fisicoquímicos industriales, el número de variables $M$ por observación puede variar desde unas cuantas hasta **decenas o incluso cientos**.

Cuando el número de variables es elevado, surgen **dificultades tanto logísticas como matemáticas** al trabajar con datos multivariables. Desde el punto de vista operativo, aumenta la complejidad del almacenamiento, la visualización y la interpretación de la información. Desde el punto de vista matemático, aparecen problemas asociados a la correlación, la redundancia y la inestabilidad numérica de los modelos.

La **compresión de datos** es el proceso mediante el cual un conjunto de datos de alta dimensionalidad se transforma en una representación que utiliza **menos variables**, pero que conserva, en forma condensada, la mayor parte de la información relevante. Desde una perspectiva estadística, una representación comprimida reduce el número de variables, **requiere menos almacenamiento** y **consume menos recursos computacionales**. Desde una perspectiva analítica, permite **filtrar información irrelevante, redundante o indeseada**, facilitando el desempeño de las técnicas de modelación que se aplican posteriormente.

Existen múltiples métodos de compresión de datos utilizados en distintas áreas técnicas, como las **transformadas de Fourier**, las **wavelets** y métodos afines, ampliamente empleados en espectroscopía, cromatografía y otros problemas analíticos específicos.

Sin embargo, existe un tipo particular de compresión de datos que se ha convertido en el **fundamento de los métodos quimiométricos** y que resulta especialmente ventajoso para el **control estadístico de procesos fisicoquímicos multivariables**: el **Análisis de Componentes Principales (PCA)**.

## Los detalles de PCA

La siguiente terminología y vocabulario pueden resultar nuevos para algunos lectores. Eso está bien porque esta lectura es para ellos y no para especialistas experimentados que la pueden encontrar básica, elemental y aburrida.

PCA es un método de compresión de datos que se considera rutinario y elemental en estadística multivariable. ¿Qué nos ofrece? Lo que hace es reducir un conjunto de datos obtenido a partir de $M$ variables medidas a lo largo de $N$ observaciones. Esta reducción consiste en producir una representación más simple que, en muchos casos, utiliza un número mucho menor de “variables comprimidas” ( $A≪M$). En algunos casos la compresión es limitada ( $A<M$ ) y esto es una clara señal de que el control no será alcanzable y la causa es solo una: el proceso no es repetible y la falta de compresión suficiente es evidencia de ello. Si el proceso estuviera en control, los datos se comprimirían.

Estas variables comprimidas reciben el nombre de **componentes principales** (PCs).

La formulación matemática estándar de PCA se expresa como:

### $$ \boldsymbol{X = TP^t + E} $$Donde:

-   $\boldsymbol{X}$ es la matriz de datos original de dimensiones $N×M$

-   $\boldsymbol{T}$ es una matriz $N×A$ que contiene los *scores* de los componentes principales

-   $\boldsymbol{P}$ es una matriz $M×A$ que contiene los *loadings* de los componentes principales

-   $\boldsymbol{E}$ es una matriz $N×M$ de residuales

Para el lector que comienza a explorar esta lectura, los términos y símbolos matemáticos pueden parecer extraños o innecesariamente abstractos. Es importante aclarar desde el inicio que no se trata de nada más que **notación básica de álgebra lineal**, utilizada para describir algo muy familiar: **tablas de datos**.

En estas tablas, las **columnas representan variables** y las **filas representan valores medidos sucesivamente**, una observación tras otra. Esa tabla completa es lo que llamamos la matriz $\boldsymbol{X}$.

Cada fila de esa matriz contiene todos los valores medidos en un instante, corrida u observación particular. En química analítica, a esto se le suele llamar una *corrida*. En estadística se le llama *observación*. En álgebra lineal, se le llama *vector*. En términos prácticos, todos estos términos describen exactamente lo mismo: **una fila de la tabla de datos**. Cada celda de esa fila es un dato individual.

Así, en términos de nomenclatura:

-   $\boldsymbol{X}$ es la **matriz de datos completa**,

-   $x_1, x_2, x_3, ... x_n$ son los **vectores** (observaciones, corridas) que conforman la muestra multivariable, desde la primera hasta la última.

La matriz $\boldsymbol{X}$ no es algo externo al proceso: **ella es la muestra**. Toda la información disponible sobre el estado del sistema fisicoquímico para construir nuestro **procedimiento de control** está contenida ahí.

Después del tratamiento por PCA, la información contenida en $\boldsymbol{X}$ se reorganiza en un nuevo conjunto de variables, los **componentes principales**, que denotamos como $A$.

El lector sin experiencia previa no debe perder de vista lo esencial: **la información que nos permite evaluar la estabilidad general de un proceso está contenida en la varianza**. Todos los cambios fisicoquímicos de interés (reacciones, desviaciones operativas, degradaciones, fallas incipientes) se manifiestan inevitablemente como cambios en los datos, y esos datos siempre terminan organizados en una matriz.

Las variables originales viven en un **espacio euclidiano**, donde sus correlaciones y colinealidades están presentes y son visibles. Sin embargo, cuando aplicamos PCA, el análisis de datos y la calibración multivariable **ya no ocurren en ese espacio original**, sino en lo que se conoce como el **espacio ortogonal**.

En este nuevo espacio, las variables originales son reemplazadas por los componentes principales:

-   $A_1, A_2, A_3... etc$

Cada uno de estos componentes contiene información de variación del proceso, cuantificada mediante su correspondiente **eigenvalor**:

-   $\lambda_1, \lambda_2, \lambda_3... etc$

Cada eigenvalor representa cuánta varianza explica su componente principal asociado. Lo peculiar (y fundamental) es que **los componentes principales no están correlacionados entre sí**. Esta propiedad es precisamente la que hace a PCA tan atractivo y tan poderoso: el problema de la correlación entre variables prácticamente desaparece.

Más adelante veremos por qué esta característica es crítica para el monitoreo y control estadístico de procesos.

Desde un punto de vista operativo:

-   Los **scores** pueden interpretarse como las magnitudes de cada una de las variables comprimidas $A$ para todas las observaciones $N$. Es decir, describen cómo se comporta cada observación dentro del espacio PCA. Esta información está contenida en la matriz $\boldsymbol{T}$.

-   Los **loadings**, por su parte, representan la traducción entre las variables originales $M$ de la matriz $\boldsymbol{X}$ y los componentes principales $A$. Esta relación está contenida en la matriz $\boldsymbol{P}$.

En términos simples, los loadings describen **cómo cada componente principal se expresa en función de las variables físicas originales**. Son el puente matemático que conecta el espacio abstracto ortogonal con el sistema fisicoquímico material.

Esta transformación no elimina información ni la distorsiona. La reorganiza. Y esa reorganización es la condición necesaria para que el control estadístico multivariable sea matemáticamente válido, operacionalmente estable y científicamente confiable.

Desde esta perspectiva, PCA no destruye la información contenida en $\boldsymbol{X}$, sino que la transforma a un espacio equivalente pero matemáticamente más manejable. Es una conversión desde el espacio original de mediciones físicas a un espacio abstracto donde la información se encuentra reorganizada y desacoplada.

En la implementación más común del algoritmo PCA se cumplen dos propiedades fundamentales:

-   Los *loadings* de cada componente principal están normalizados y son ortogonales entre sí:

### $$
\boldsymbol{P^tP = I}
$$

-   Los *scores* de los componentes principales también son ortogonales entre sí, y su matriz de covarianzas está dada por:

### $$
\frac{\mathbf{T}^\mathsf{T}\mathbf{T}}{N-1}
=
\operatorname{diag}(\boldsymbol{\lambda})
$$

donde $\boldsymbol{I}$ es la matriz de identidad y $\boldsymbol{diag⁡(λ)}$ es una matriz diagonal que contiene los **autovalores** (eigenvalues) de los componentes principales.

## ¿Qué significa todo esto?

Vistas desde la estadística clásica, cada variable aporta una cantidad de varianza $\sigma^2$.

De igual manera, cada autovalor $\lambda_a$​ representa la cantidad de varianza de los datos originales, pero ahora explicada por el componente principal $A$. Como es natural, la suma de todas las varianza, que en este caso son todos los autovalores, corresponde al 100 % de la varianza total del sistema. Así, un pequeño número de componentes principales puede capturar la mayor parte, o la totalidad de la variabilidad relevante del proceso. Toda la información que nos interesa analizar está contenida ahí.

¿Cómo?

Conceptualmente, cada variable original $X_1, X_2, X_3,..., X_n$ puede entenderse como un eje en un espacio euclidiano. Al contener muchas variables, se dice que este espacio es de alta dimensión. Algo que sucede de manera natural es que todas las correlaciones entre variables se preservan. Sin embargo, esa misma riqueza estructural introduce colinealidades que vuelven inestables e inválidos muchos modelos estadísticos y eso es justamente lo que obliga a proponer y explorar mecanismos alternativos para procesar y analizar y poder trabajar con nuestros datos en aploicaciones como la construcción de procedimientos de control estadístico. PCA actúa precisamente desacoplando las correlaciones entre estas dimensiones y reorganizando la información en un nuevo sistema de ejes ortogonales.

Al decir que "se busca desacoplar correlaciones entre variables" el lector rápidamente puede detectar que esto no tendría sentido físico e iría contra toda intuición científica: ¿para qué querríamos eliminar las correlaciones entre variables, si es precisamente allí donde se encuentra nuestra información?

Aunque las nuevas variables (los componentes principales) no tienen significado físico directo, esto no representa una desventaja ni una pérdida de información. Es simplemente el paso de una representación material a una representación matemática. Este diseño permite que los modelos estadísticos funcionen correctamente.

¿Cómo?

La información de la variación, magnitud, orientación y correlación pasa de estar contenida en cada variable original $X_n$, a cada componente pricipal $A$.

## ¿Cómo ayuda esto al monitoreo y control?

En un contexto de monitoreo y control estadístico de procesos, el uso de PCA es crítico porque permite **evitar el problema de intercorrelación**. Utilizar componentes principales como variables de monitoreo en lugar de las variables originales en la matriz $\boldsymbol{X}$ impide:

-   La inflación artificial de alarmas causada por variables correlacionadas que se desvían de manera coherente.

-   El enmascaramiento de fallas reales, donde desviaciones pequeñas pero coordinadas pasan desapercibidas en análisis univariables.

-   La violación de supuestos fundamentales de independencia requeridos por los estadísticos clásicos.

-   La pérdida de capacidad diagnóstica al no poder separar variaciones sistemáticas de ruido o de comportamientos fuera del modelo.

-   La dependencia excesiva del criterio subjetivo del operador para interpretar señales múltiples y contradictorias.

En el espacio PCA, el comportamiento del proceso puede descomponerse formalmente en dos tipos de variación: la variación sistemática capturada por el modelo (medida típicamente con $T^2$) y la variación no explicada por el modelo (medida con $Q$ o **SPE**). Esta separación es imposible sin compresión y ortogonalización.

Antes de PCA, el control se realiza sobre muchas variables colineales, con estadística inválida y resultados frágiles. Después de PCA, el control se realiza sobre pocas variables ortogonales, con estadística válida, interpretación clara y capacidad diagnóstica real.

No se trata de simplificar el proceso. Se trata de **hacerlo visible**.

No se trata de perder información. Se trata de **organizarla de forma que el control estadístico funcione** sin verse afectado por las covarianzas del sistema.

En este sentido, PCA no es una opción elegante ni una herramienta académica más. Es una **condición necesaria** para cualquier sistema serio de monitoreo y diagnóstico estadístico de procesos fisicoquímicos multivariables al operar con componentes principales en lugar de variables originales.

## Los residuales

Cuando aplicamos métodos de compresión de datos y seleccionamos únicamente una fracción de la información disponible, esto implica necesariamente **ignorar parte de la variación** contenida en la matriz de datos originales $\boldsymbol{X}$. Esa fracción de variación no modelada se almacena en la matriz de **residuales**, denotada como $\boldsymbol{E}$.

Una vez calculada la matriz de *scores* estimados $\boldsymbol{\hat{T}}$ correspondiente a los componentes principales $A$ seleccionados, y la matriz de *loadings* estimados $\boldsymbol{\hat{P}}$ asociada a esos mismos componentes, los residuales (es decir, la parte de los datos que **no es explicada por el modelo PCA**) se estiman como:

### $$
\boldsymbol{\hat{E} = X - \hat{T}\hat{P}^t}
$$

Desde un punto de vista conceptual, esta ecuación expresa que los datos originales $\boldsymbol{X}$ se descomponen en dos partes:

-   **Una parte modelada**, capturada por el subespacio definido por los $A$ componentes principales seleccionados.

-   **Una parte no modelada**, contenida en los residuales $\boldsymbol{\hat{E}}$, que representa variación que el modelo PCA no explica.

## El problema práctico: ¿cuántos componentes conservar?

En la práctica, surge un desafío central: la decisión de **cuántos componentes principales** $A$ conservar en el modelo PCA. Esta decisión **no es puramente objetiva**. Aunque existen reglas heurísticas y criterios estadísticos ampliamente utilizados, la selección de $A$ sigue siendo, en esencia, una decisión de compromiso.

El objetivo de esta decisión es equilibrar dos necesidades contrapuestas:

-   **Explicar la mayor cantidad posible de la variación** contenida en los datos originales.

-   **Evitar la inclusión de ruido** en el modelo, ya que incorporar demasiados componentes conduce inevitablemente al **sobreajuste**, es decir, a modelar variación aleatoria que no representa comportamiento estructural del proceso.

Un modelo PCA con pocos componentes puede resultar insuficiente y dejar fuera información relevante del proceso. En cambio, un modelo con demasiados componentes pierde su capacidad de generalización y se vuelve sensible al ruido, lo cual degrada su valor como herramienta de monitoreo y diagnóstico.

Esta tensión entre **explicación y parsimonia** es inherente a todos los modelos empíricos multivariables y constituye uno de los puntos críticos en la construcción de calibraciones quimiométricas robustas.

## El principal beneficio de PCA en el control estadístico de procesos fisicoquímicos multivariables

En su uso tradicional (y limitado) PCA suele emplearse como una herramienta exploratoria en contextos genéricos de "ciencia de datos", sin explotar sus capacidades diagnósticas más potentes.

Sin embargo, en un contexto de **diagnóstico fisicoquímico aplicado al control estadístico de procesos multivariables**, gobernados por sensores, actuadores electrónicos y tecnologías analíticas de proceso (PAT), PCA se convierte en una **herramienta de predicción de la estabilidad del proceso de transformación fisicoquímica**.

Un caso típico de esta aplicación ocurre cuando se desea determinar si los datos obtenidos **en tiempo presente** son aceptables o inaceptables al compararlos contra un conjunto de referencia utilizado para construir una **calibración multivariable**.

Las decisión es simple porque es binaria: si los datos son aceptables, el proceso continúa. De lo contrario, se requiere intervención.

## ¿Cómo se realiza este procedimiento?

El enfoque correcto consiste en construir un modelo PCA utilizando una cantidad suficiente de observaciones multivariables provenientes de un conjunto de datos de referencia que represente condiciones operativas normales del proceso y cuya producción cumpla con los criterios de conformidad del producto.

Una vez construido el modelo, este se aplica a nuevas corridas de datos $\boldsymbol{x}$​ conocidas individualmente como el **"*perfil analítico*"** y que son generadas conforme el proceso continúa en operación.

Este procedimiento exige una secuencia de pasos bien definida, es decir, un **algoritmo** y que se puede programar fácilmente en una computadora.

### Preprocesamiento del nuevo vector

Cada nuevo vector de observación debe ser preprocesado mediante **autoescalamiento**. El procedimiento consiste en restar el vector medio $\boldsymbol{\bar{x}}$ y dividir entre el vector de desviaciones estándar $\boldsymbol{s}$, para obtener:

### $$
\boldsymbol{x_p = \frac{x - \bar{x}}s} 
$$

### Proyección al espacio PCA

El siguiente paso es proyectar el nuevo vector escalado $\boldsymbol{x}_p$​ al espacio del modelo PCA mediante los *loadings* $\boldsymbol{P}$:

### $$
\boldsymbol{\hat{t}_p} = \boldsymbol{x_p}\boldsymbol{P}
$$

En álgebra lineal, esta operación se conoce como la **proyección de la observación al subespacio PCA**.

### Reconstrucción y cálculo del residual

Una vez obtenidos los *scores* proyectados y contenidos en el vector $\boldsymbol{\hat{t}_p}$, nuestro algoritmo ahora calcula la respuesta $\boldsymbol{\hat{x}_p}$ estimada por el modelo PCA:

### $$ \boldsymbol{\hat{x}_p} = \boldsymbol{\hat{t}_p}\boldsymbol{P} $$

El residual $\boldsymbol{\hat{e}_p}$ asociado a la nueva observación se obtiene entonces como:

### $$
\boldsymbol{\hat{e}_p = x_p - \hat{x}_p} 
$$

Conceptualmente, el residual representa la porción de la medición multivariable que el modelo PCA **no puede explicar**. Si el modelo explicara también esta fracción, estaría modelando ruido y conduciría inevitablemente al sobreajuste.

## Métricas de diagnóstico: $T^2$ y $Q$

El propósito de estos cálculos es reconocer que tanto los *scores* $\boldsymbol{\hat{t}}_p$​ como los residuales $\boldsymbol{\hat{e}}_p$ constituyen **métricas de predicción**. A partir de ellos se construyen dos estadísticos fundamentales para la evaluación de la calidad y la detección válida de anomalías en procesos fisicoquímicos.

### Estadístico $T^2$ de Hotelling

En aplicaciones industriales de transformaciones fisicoquímicas (tal como se presenta en la literatura quimiométrica especializada) el estadístico $T^2$ adopta una forma que, aunque algebraicamente equivalente a la definición clásica de la estadística multivariable genérica, resulta operacionalmente más adecuada para su implementación en sistemas de monitoreo. Puede expresarse de dos maneras equivalentes:

### $$ \boldsymbol{T^2 = \hat{t}_p * diag(\lambda)^{-1} * \hat{t}^t_p} $$

y, de forma escalar,

### $$ \boldsymbol{T^2 = \Sigma^A_{a=1} \frac{t^2_a}{\lambda_a}} $$

donde $\lambda_a$​ representa la varianza asociada al componente principal $A$.

### Estadístico de residuales $Q$

De manera análoga, el estadístico de residuales $Q$ (también conocido como *Squared Prediction Error* (SPE)) puede definirse de dos formas equivalentes:

### $$ \boldsymbol{Q = \hat{e}_p *\hat{e}^t_p} $$

o bien,

### $$ \boldsymbol{Q = \Sigma^J_{j=1} {e^2_{ij}}} $$

### Interpretación conjunta

Desde un punto de vista conceptual, el estadístico $T^2$ mide la **lejanía multivariable de una observación dentro del subespacio modelado por el PCA**, mientras que el estadístico $Q$ cuantifica la **cantidad de variación que queda fuera de dicho subespacio**, comúnmente interpretada como falta de ajuste.

Es completamente válido interpretar la varianza total de un sistema estocástico como una suma que representa el 100% de la información disponible. En este sentido, los estadísticos $T^2$ y $Q$ capturan **fracciones complementarias de esa misma varianza total**. No se trata de omitir información, sino precisamente de **procesarla en su totalidad**.

Por esta razón, ambos estadísticos son necesarios para una evaluación completa de las anomalías detectables durante la operación del proceso:

-   $T^2$ concentra toda la variación **dentro** del modelo PCA,

-   mientras que $Q$ captura toda la variación que queda **fuera** de él.

## Límites de confianza y lanzamiento del procedimiento de control

Antes de operar un modelo PCA como herramienta de monitoreo, es **necesario y obligatorio** establecer límites de confianza que funcionen como **límites de control** para ambos estadísticos de diagnóstico: $T^2$ y $Q$.

Existen diversos métodos para determinar estos límites; sin embargo, en términos generales, todos requieren dos elementos fundamentales:

-   Los valores de $T^2$ y $Q$ obtenidos a partir del conjunto de calibración, es decir, de datos que representan condiciones operativas aceptables del proceso.

-   La especificación explícita del nivel de confianza deseado (95%, 99%, 99.9%, etc.), el cual es definido por el cliente en función del compromiso requerido entre **sensibilidad** (capacidad de detección de anomalías) y **especificidad** (evitar falsas alarmas) del sistema de monitoreo.

Una vez que estos límites han sido calibrados y que la robustez del modelo PCA ha sido validada, el modelo puede desplegarse formalmente como un **procedimiento de control estadístico multivariable**, capaz de detectar outliers en tiempo real durante la operación normal del proceso.

La obra de J. Edward Jackson, *A User’s Guide to Principal Components*, resulta particularmente ilustrativa en este contexto para los profesionales de la química industrial aplicada. El propio Jackson reconoce que gran parte del desarrollo práctico de estas metodologías surgió durante su trabajo en la industria química de Kodak desde la década de los 40s, en estrecha colaboración con **químicos interesados en el desarrollo de herramientas estadísticas avanzadas, para aplicarlas al control y mejora de la calidad de productos químicos.**

Jackson enfatiza que el límite de control del estadístico $T^2$ está íntimamente relacionado con la distribución de probabilidad $F$, y se calcula como:

### $$
\boldsymbol{T^2_{p, n, \alpha} = \frac{p(n-1)}{n - p} F_{p, n-p, \alpha}}
$$

donde $\boldsymbol{p}$ es el número de componentes principales retenidos en el modelo, $\boldsymbol{n}$ es el número de observaciones de calibración y $\boldsymbol{\alpha}$ es el nivel de significancia seleccionado.

De manera análoga, Jackson presenta la expresión para el valor crítico del estadístico residual $\boldsymbol{Q}$:

### $$
\boldsymbol{Q_\alpha = \theta_1 [\frac{c_\alpha\sqrt{2\theta_2h^2_0}}{\theta_1} +\frac{\theta_2h_0(h_0-1)}{\theta^2_1} + 1]^{t/h_0}}
$$

donde los términos $\boldsymbol{\theta_1}$​, $\boldsymbol{\theta_2}$​ y $\boldsymbol{h_0}$​ dependen de los eigenvalores no incluidos en el modelo, y $\boldsymbol{c_\alpha}$​ es un valor crítico asociado a una distribución aproximadamente normal.

Jackson hace una aclaración particularmente relevante desde el punto de vista industrial: la aproximación normal de $\boldsymbol{c_\alpha}$​ es válida **independientemente de si se han incluido o no todos los componentes principales estadísticamente significativos** en el modelo PCA. Esta propiedad refuerza de manera decisiva la utilidad práctica del estadístico $\boldsymbol{Q}$ como una métrica robusta para el monitoreo de procesos reales, donde la **parsimonia del modelo** es tan importante como su **capacidad de detección**.

En conjunto, estos límites de confianza constituyen el paso final que coloca a los modelos PCA en su debido y justo lugar: dejan de ser un simple “*algoritmo de aprendizaje no supervisado*”, una desafortunada descripción que surge de la perspectiva reduccionista de la popularmente llamada *ciencia de datos*. En ese ámbito, los modelos PCA suelen interpretarse como un procedimiento intermedio y arcaico, una herramienta descriptiva primitiva de patrones pero no de conclusiones, en la que el significado material y la interpretabilidad de los datos supuestamente “se pierden”.

Para la industria química, en cambio, los modelos PCA se revelan y prevalecen como lo que verdaderamente son: **sistemas analíticos formales de datos de medición para el control estadístico multivariable de procesos fisicoquímicos**, matemáticamente trazables, operacionalmente robustos y plenamente auditables, precisamente porque están rigurosamente fundamentados en el álgebra lineal.

Los algoritmos genéricos de *machine learning* promovidos desde la ciencia de datos no pueden ofrecer un nivel comparable de escrutinio metodológico ni de trazabilidad matemática.

Se trata, en el caso del PCA, de sistemas diseñados con transparencia para su despliegue y monitoreo continuo en entornos industriales reales, capaces de informar (observación tras observación) la estabilidad operativa de las relaciones vinculantes entre todas las variables del proceso.

No existe un método más riguroso, transparente y libre de cajas negras para asegurar la calidad del producto y garantizar la satisfacción del cliente.

## El diagnóstico de causas raíz

No obstante, para que un procedimiento de control estadístico multivariable sea verdaderamente completo, es indispensable atender la siguiente pregunta fundamental:

**¿Cuál o cuáles variables causaron la desviación observada?**

El mecanismo quimiométrico diseñado específicamente para responder esta pregunta son las **contribuciones** $t$ y las **contribuciones** $q$.

Es precisamente en este punto donde el proceso deja de comportarse como una *caja negra* (como ocurre con muchos algoritmos de *machine learning*) y comienza a **explicarse a sí mismo**, proporcionando información diagnóstica interpretable desde el punto de vista fisicoquímico.

Es importante aclarar que, en el desarrollo de modelos PCA para el control estadístico multivariable de procesos fisicoquímicos, existen **dos tipos claramente diferenciados de outliers**:

1.  **Outliers presentes durante la etapa de calibración del modelo**, los cuales pueden conducir al desarrollo de modelos multivariables subóptimos si no son identificados y tratados adecuadamente.

2.  **Outliers detectados durante la etapa de operación (despliegue) del modelo**, los cuales pueden resultar igual o incluso más dañinos.

Esto último ocurre porque cualquier resultado de predicción será necesariamente **inválido** al obtenerlo tras aplicar un modelo empírico a una nueva observación que proviene de un **estado fisicoquímico extraño**, es decir, **no representado y por tanto desconocido** en los datos de calibración.

Cualquier intento de aplicar un modelo de calibración multivariable a observaciones nuevas en tales estados inapropiados resultará en predicciones **evidentemente inexactas**.

Como consecuencia, es crítico evaluar continuamente si las muestras provenientes del proceso de transformación son **aptas para su uso con el modelo empírico**. En modelos construidos mediante PCA, PLS, PCR u otros métodos basados en factores latentes, el mecanismo convencional para este tipo de *monitores de salud del modelo* se encuentra **intrínsecamente incorporado en el propio modelo de calibración**.

Tanto durante la calibración como durante la operación del método, un valor anormalmente elevado del estadístico $T^2$ indica que la observación exhibe una conducta irregular que **aún reside dentro del espacio multivariado del modelo**, pero en una región distante de las observaciones típicas que definen el modelo de calibración PCA contra el cual se comparan las nuevas observaciones generadas por el proceso.

En contraste, un valor residual $Q$ anormalmente elevado indica que la observación contiene **información significativamente distinta** a la capturada por el modelo PCA de calibración, es decir, información que **no puede ser explicada por los componentes principales retenidos**.

En cualquiera de ambos casos, cualquier resultado generado a partir de una observación que presente valores elevados de $T^2$ y/o $Q$ debe considerarse una **predicción inválida**.

Durante la operación real de procesos industriales, el número de observaciones multivariables fácilmente supera las decenas o centenas. Dado el volumen de cálculo involucrado, el tiempo limitado disponible y la inevitable presencia de error humano, **su monitoreo debe ser automatizado mediante software especializado**. Nunca debe realizarse de forma manual ni con plantillas configuradas en software genérico de hojas de cálculo. El riesgo de error en el cómputo y de configuraciones frágiles en el diseño del algoritmo es demasiado elevado.

El uso de $T^2$ y $Q$ como métricas para la detección de outliers en el control estadístico de procesos multivariables fisicoquímicos constituye un claro ejemplo de **monitores de salud del sistema bajo observación**. Estas métricas emergen directamente de un modelo de predicción (ya sea PCA, PLS o PCR) y establecen el estándar cuantitativo contra el cual cada nueva observación es comparada, permitiendo una decisión binaria: **aceptar o rechazar la observación**, y actuar en consecuencia.

Por esta razón, en quimiometría, $T^2$ y $Q$ son consideradas las **métricas de élite en la detección de anomalías**.

Adicionalmente, para cualquier nueva observación que produzca valores elevados o sospechosos de $T^2$ y/o $Q$, es posible calcular las **contribuciones individuales de cada variable fisicoquímica** que impactan dichos estadísticos. Estas reciben el nombre de **contribuciones** $t$ y **contribuciones** $q$, cuyas definiciones son:

### $$
\boldsymbol{t_{cont} = \hat{t}_p \lambda^{1/2} P^t}
$$

### $$
\boldsymbol{q_{cont} = \hat{e}_p} 
$$

Estas contribuciones resultan particularmente útiles para evaluar la **naturaleza específica de la causa (o causas) de la anomalía** detectada durante el monitoreo del proceso.

El objetivo es único y claro:

**Si se desea determinar la naturaleza específica o las causas raíz de observaciones anómalas, entonces las contribuciones** $t$ y $q$ asociadas a aquellas lecturas donde se observaron valores elevados de $T^2$ y $Q$ proporcionan la información diagnóstica más poderosa y útil disponible actualmente en la ciencia quimiométrica.

## Conclusión

Tras esta exposición, el lector podría concluir que el rigor matemático aquí presentado pertenece exclusivamente al ámbito académico o al ejercicio teórico dentro del aula. Ese criterio es incorrecto.

Nada de lo expuesto corresponde a una teoría fisicoquímica. Todo el desarrollo presentado constituye, en realidad, la base estrictamente matemático-estadística necesaria para construir algoritmos programables destinados a la producción de software de monitoreo en tiempo real. En ese contexto, la aplicación resuelve el único problema industrial que verdaderamente importa:

**El aseguramiento objetivo y continuo de la calidad del producto para la satisfacción del cliente.**

Esto exige la definición de una regla de decisión clara, binaria y auditable:

**Aceptación o rechazo de una observación, y la identificación de sus causas.**

¿Y qué hace que dicha decisión sea auditable?

1.  La trazabilidad matemática completa del método que da origen al modelo.
2.  La expresión gráfica y visual que permite rapidez y claridad intuitiva al tomar cada decisión.

Fundamentar y demostrar cada decisión tomada bajo un mecanismo que permita justificarla sin ambigüedades, sin arbitrariedades y sin subjetividades es lo que protege la reputación profesional. No existe otra forma. (Si el lector conoce alguna alternativa igualmente rigurosa, le pedimos que nos la haga saber; nos interesaría enormemente conocerla).

Por esta razón, en un contexto de control estadístico multivariable de procesos, los algoritmos genéricos de *machine learning* no constituyen una opción responsable ni científica: al operar como cajas negras, el escrutinio de la interacción entre variables queda oculto, y la rendición de cuentas con transparencia metodológica se vuelve imposible.

Esto no significa que los algoritmos de *machine learning* ofrecidos por la ciencia de datos carezcan de utilidad industrial. Todo lo contrario: su capacidad predictiva puede ser notable. Su limitación fundamental es otra: **no son matemáticamente transparentes, ni pueden serlo**, porque no han sido diseñados con ese propósito.

Al final, el lector elige:

-   metodologías multivariables reconocidas, trazables y abiertas;

-   metodologías multivariables novedosas, genéricas y cerradas;

-   o una combinación consciente y responsable de ambas.

Por estas razones, esta exposición no debe interpretarse como un ejercicio teórico-académico, sino como lo que realmente es:

**Una metodología estrictamente aplicada para la industria de transformaciones fisicoquímicas**, diseñada para operar procesos reales bajo condiciones reales, con criterios objetivos, reproducibles, defendibles y plenamente auditables.

Invitamos al lector a enviarnos sus comentarios, preguntas y sugerencias.
